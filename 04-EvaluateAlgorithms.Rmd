# 04 Evaluate Algorithms {@04 EvaluateAlgorithms}
## a) Spot-Check Algorithms {a) Spot-Check Algorithms}
```{r algorithm-spot-check, message=FALSE, warning=FALSE}
# Minimum examples of algorithm spot checks, both direct and with caret.

# Linear Regression

# lm direct use

# load the library
library(mlbench)
# load data
data(BostonHousing)
# fit model
fit <- lm(medv~., BostonHousing)
# summarize the fit
print(fit)
# make predictions
predictions <- predict(fit, BostonHousing)
# summarize accuracy
mse <- mean((BostonHousing$medv - predictions)^2)
print(mse)

# lm in caret

# load libraries
library(caret)
library(mlbench)
# load dataset
data(BostonHousing)
# train
set.seed(7)
control <- trainControl(method="cv", number=5)
fit.lm <- train(medv~., data=BostonHousing, method="lm", metric="RMSE", preProc=c("center", "scale"), trControl=control)
# summarize fit
print(fit.lm)
# Logistic Regression

# glm direct use

# load the library
library(mlbench)
# Load the dataset
data(PimaIndiansDiabetes)
# fit model
fit <- glm(diabetes~., data=PimaIndiansDiabetes, family=binomial(link='logit'))
# summarize the fit
print(fit)
# make predictions
probabilities <- predict(fit, PimaIndiansDiabetes[,1:8], type='response')
predictions <- ifelse(probabilities > 0.5,'pos','neg')
# summarize accuracy
table(predictions, PimaIndiansDiabetes$diabetes)
# glm in caret

# load libraries
library(caret)
library(mlbench)
# Load the dataset
data(PimaIndiansDiabetes)
# train
set.seed(7)
control <- trainControl(method="cv", number=5)
fit.glm <- train(diabetes~., data=PimaIndiansDiabetes, method="lm", metric="Accuracy", preProc=c("center", "scale"), trControl=control)
# summarize fit
print(fit.glm)

# Linear Discriminant Analysis

# lda direct

# load the libraries
library(MASS)
library(mlbench)
# Load the dataset
data(PimaIndiansDiabetes)
# fit model
fit <- lda(diabetes~., data=PimaIndiansDiabetes)
# summarize the fit
print(fit)
# make predictions
predictions <- predict(fit, PimaIndiansDiabetes[,1:8])$class
# summarize accuracy
table(predictions, PimaIndiansDiabetes$diabetes)

# lda in caret

# load libraries
library(caret)
library(mlbench)
# Load the dataset
data(PimaIndiansDiabetes)
# train
set.seed(7)
control <- trainControl(method="cv", number=5)
fit.lda <- train(diabetes~., data=PimaIndiansDiabetes, method="lda", metric="Accuracy", preProc=c("center", "scale"), trControl=control)
# summarize fit
print(fit.lda)

# Regularized Regression

# glmnet direct classification

# load the library
library(glmnet)
library(mlbench)
# load data
data(PimaIndiansDiabetes)
x <- as.matrix(PimaIndiansDiabetes[,1:8])
y <- as.matrix(PimaIndiansDiabetes[,9])
# fit model
fit <- glmnet(x, y, family="binomial", alpha=0.5, lambda=0.001)
# summarize the fit
print(fit)
# make predictions
predictions <- predict(fit, x, type="class")
# summarize accuracy
table(predictions, PimaIndiansDiabetes$diabetes)

# glmnet direct regression

# load the libraries
library(glmnet)
library(mlbench)
# load data
data(BostonHousing)
BostonHousing$chas <- as.numeric(as.character(BostonHousing$chas))
x <- as.matrix(BostonHousing[,1:13])
y <- as.matrix(BostonHousing[,14])
# fit model
fit <- glmnet(x, y, family="gaussian", alpha=0.5, lambda=0.001)
# summarize the fit
print(fit)
# make predictions
predictions <- predict(fit, x, type="link")
# summarize accuracy
mse <- mean((y - predictions)^2)
print(mse)

# glmnet in caret classification

# load libraries
library(caret)
library(mlbench)
library(glmnet)
# Load the dataset
data(PimaIndiansDiabetes)
# train
set.seed(7)
control <- trainControl(method="cv", number=5)
fit.glmnet <- train(diabetes~., data=PimaIndiansDiabetes, method="glmnet", metric="Accuracy", preProc=c("center", "scale"), trControl=control)
# summarize fit
print(fit.glmnet)

# glmnet in caret regression

# load libraries
library(caret)
library(mlbench)
library(glmnet)
# Load the dataset
data(BostonHousing)
# train
set.seed(7)
control <- trainControl(method="cv", number=5)
fit.glmnet <- train(medv~., data=BostonHousing, method="glmnet", metric="RMSE", preProc=c("center", "scale"), trControl=control)
# summarize fit
print(fit.glmnet)

# k-Nearest Neighbors

# knn direct classification

# load the libraries
library(caret)
library(mlbench)
# Load the dataset
data(PimaIndiansDiabetes)
# fit model
fit <- knn3(diabetes~., data=PimaIndiansDiabetes, k=3)
# summarize the fit
print(fit)
# make predictions
predictions <- predict(fit, PimaIndiansDiabetes[,1:8], type="class")
# summarize accuracy
table(predictions, PimaIndiansDiabetes$diabetes)

# knn direct regression

# load the libraries
library(caret)
library(mlbench)
# load data
data(BostonHousing)
BostonHousing$chas <- as.numeric(as.character(BostonHousing$chas))
x <- as.matrix(BostonHousing[,1:13])
y <- as.matrix(BostonHousing[,14])
# fit model
fit <- knnreg(x, y, k=3)
# summarize the fit
print(fit)
# make predictions
predictions <- predict(fit, x)
# summarize accuracy
mse <- mean((BostonHousing$medv - predictions)^2)
print(mse)

# knn in caret classification

# load libraries
library(caret)
library(mlbench)
# Load the dataset
data(PimaIndiansDiabetes)
# train
set.seed(7)
control <- trainControl(method="cv", number=5)
fit.knn <- train(diabetes~., data=PimaIndiansDiabetes, method="knn", metric="Accuracy", preProc=c("center", "scale"), trControl=control)
# summarize fit
print(fit.knn)

# knn in caret regression

# load libraries
library(caret)
data(BostonHousing)
# Load the dataset
data(BostonHousing)
# train
set.seed(7)
control <- trainControl(method="cv", number=5)
fit.knn <- train(medv~., data=BostonHousing, method="knn", metric="RMSE", preProc=c("center", "scale"), trControl=control)
# summarize fit
print(fit.knn)

# Naive Bayes

# naive bayes direct

# load the libraries
library(e1071)
library(mlbench)
# Load the dataset
data(PimaIndiansDiabetes)
# fit model
fit <- naiveBayes(diabetes~., data=PimaIndiansDiabetes)
# summarize the fit
print(fit)
# make predictions
predictions <- predict(fit, PimaIndiansDiabetes[,1:8])
# summarize accuracy
table(predictions, PimaIndiansDiabetes$diabetes)

# naive bayes in caret

# load libraries
library(caret)
library(mlbench)
# Load the dataset
data(PimaIndiansDiabetes)
# train
set.seed(7)
control <- trainControl(method="cv", number=5)
fit.nb <- train(diabetes~., data=PimaIndiansDiabetes, method="nb", metric="Accuracy", trControl=control)
# summarize fit
print(fit.nb)

# Support Vector Machine

# SVM direct classification

# load the libraries
library(kernlab)
library(mlbench)
# Load the dataset
data(PimaIndiansDiabetes)
# fit model
fit <- ksvm(diabetes~., data=PimaIndiansDiabetes, kernel="rbfdot")
# summarize the fit
print(fit)
# make predictions
predictions <- predict(fit, PimaIndiansDiabetes[,1:8], type="response")
# summarize accuracy
table(predictions, PimaIndiansDiabetes$diabetes)

# SVM direct regression

# load the libraries
library(kernlab)
library(mlbench)
# load data
data(BostonHousing)
# fit model
fit <- ksvm(medv~., BostonHousing, kernel="rbfdot")
# summarize the fit
print(fit)
# make predictions
predictions <- predict(fit, BostonHousing)
# summarize accuracy
mse <- mean((BostonHousing$medv - predictions)^2)
print(mse)

# svmRadial in caret classification

# load libraries
library(caret)
library(mlbench)
# Load the dataset
data(PimaIndiansDiabetes)
# train
set.seed(7)
control <- trainControl(method="cv", number=5)
fit.svmRadial <- train(diabetes~., data=PimaIndiansDiabetes, method="svmRadial", metric="Accuracy", trControl=control)
# summarize fit
print(fit.svmRadial)

# svmRadial in caret regression

# load libraries
library(caret)
library(mlbench)
# Load the dataset
data(BostonHousing)
# train
set.seed(7)
control <- trainControl(method="cv", number=5)
fit.svmRadial <- train(medv~., data=BostonHousing, method="svmRadial", metric="RMSE", trControl=control)
# summarize fit
print(fit.svmRadial)


# Classification and Regression Trees

# CART direct classification

# load the libraries
library(rpart)
library(mlbench)
# Load the dataset
data(PimaIndiansDiabetes)
# fit model
fit <- rpart(diabetes~., data=PimaIndiansDiabetes)
# summarize the fit
print(fit)
# make predictions
predictions <- predict(fit, PimaIndiansDiabetes[,1:8], type="class")
# summarize accuracy
table(predictions, PimaIndiansDiabetes$diabetes)

# CART direct regression

# load the libraries
library(rpart)
library(mlbench)
# load data
data(BostonHousing)
# fit model
fit <- rpart(medv~., data=BostonHousing, control=rpart.control(minsplit=5))
# summarize the fit
print(fit)
# make predictions
predictions <- predict(fit, BostonHousing[,1:13])
# summarize accuracy
mse <- mean((BostonHousing$medv - predictions)^2)
print(mse)

# rpart in caret classification

# load libraries
library(caret)
library(mlbench)
# Load the dataset
data(PimaIndiansDiabetes)
# train
set.seed(7)
control <- trainControl(method="cv", number=5)
fit.rpart <- train(diabetes~., data=PimaIndiansDiabetes, method="rpart", metric="Accuracy", trControl=control)
# summarize fit
print(fit.rpart)

# rpart in caret regression

# load libraries
library(caret)
library(mlbench)
# Load the dataset
data(BostonHousing)
# train
set.seed(7)
control <- trainControl(method="cv", number=2)
fit.rpart <- train(medv~., data=BostonHousing, method="rpart", metric="RMSE", trControl=control)
# summarize fit
print(fit.rpart)
```

## b) Linear Regression {b) Linear Regression}
```{r linear-regression, message=FALSE, warning=FALSE}
# Ordinary Least Squares Regression

# load data
data(longley)
# fit model
fit <- lm(Employed~., longley)
# summarize the fit
print(fit)
# make predictions
predictions <- predict(fit, longley)
# summarize accuracy
mse <- mean((longley$Employed - predictions)^2)
print(mse)

# Stepwise Linear Regression

# load data
data(longley)
# fit model
base <- lm(Employed~., longley)
# summarize the fit
summary(base)
# perform step-wise feature selection
fit <- step(base)
# summarize the selected model
print(fit)
# make predictions
predictions <- predict(fit, longley)
# summarize accuracy
mse <- mean((longley$Employed - predictions)^2)
print(mse)

# Partial Least Squares Regression

# load the package
library(pls)
# load data
data(longley)
# fit model
fit <- plsr(Employed~., data=longley, validation="CV")
# summarize the fit
print(fit)
# make predictions
predictions <- predict(fit, longley, ncomp=6)
# summarize accuracy
mse <- mean((longley$Employed - predictions)^2)
print(mse)

# Principal Component Regression

# load the package
library(pls)
# load data
data(longley)
# fit model
fit <- pcr(Employed~., data=longley, validation="CV")
# summarize the fit
print(fit)
# make predictions
predictions <- predict(fit, longley, ncomp=6)
# summarize accuracy
mse <- mean((longley$Employed - predictions)^2)
print(mse)
```
## c) Penalized Linear Regression {c) Penalized Linear Regression}
```{r penalized-linear-regression, message=FALSE, warning=FALSE}
# Elastic Net

# load the package
library(glmnet)
# load data
data(longley)
x <- as.matrix(longley[,1:6])
y <- as.matrix(longley[,7])
# fit model
fit <- glmnet(x, y, family="gaussian", alpha=0.5, lambda=0.001)
# summarize the fit
print(fit)
# make predictions
predictions <- predict(fit, x, type="link")
# summarize accuracy
mse <- mean((y - predictions)^2)
print(mse)

# Ridge Regression

# load the package
library(glmnet)
# load data
data(longley)
x <- as.matrix(longley[,1:6])
y <- as.matrix(longley[,7])
# fit model
fit <- glmnet(x, y, family="gaussian", alpha=0, lambda=0.001)
# summarize the fit
print(fit)
# make predictions
predictions <- predict(fit, x, type="link")
# summarize accuracy
mse <- mean((y - predictions)^2)
print(mse)

# Least Absolute Shrinkage and Selection Operator

# load the package
library(lars)
# load data
data(longley)
x <- as.matrix(longley[,1:6])
y <- as.matrix(longley[,7])
# fit model
fit <- lars(x, y, type="lasso")
# summarize the fit
print(fit)
# select a step with a minimum error
best_step <- fit$df[which.min(fit$RSS)]
# make predictions
predictions <- predict(fit, x, s=best_step, type="fit")$fit
# summarize accuracy
mse <- mean((y - predictions)^2)
print(mse)
```
## d) Linear Classification {@d) Linear Classification}
```{r logistic-regression, message=FALSE, warning=FALSE}
#Logistic Regression

# Load the dataset
data(PimaIndiansDiabetes)
# fit model
fit <- glm(diabetes~., data=PimaIndiansDiabetes, family=binomial(link='logit'))
# summarize the fit
print(fit)
# make predictions
probabilities <- predict(fit, PimaIndiansDiabetes[,1:8], type='response')
predictions <- ifelse(probabilities > 0.5,'pos','neg')
# summarize accuracy
table(predictions, PimaIndiansDiabetes$diabetes)

# Linear Discriminant Analysis

# load the package
library(MASS)
data(iris)
# fit model
fit <- lda(Species~., data=iris)
# summarize the fit
print(fit)
# make predictions
predictions <- predict(fit, iris[,1:4])$class
# summarize accuracy
table(predictions, iris$Species)

# Partial Least Squares Discriminant Analysis

# load the package
library(caret)
data(iris)
x <- iris[,1:4]
y <- iris[,5]
# fit model
fit <- plsda(x, y, probMethod="Bayes")
# summarize the fit
print(fit)
# make predictions
predictions <- predict(fit, iris[,1:4])
# summarize accuracy
table(predictions, iris$Species)

# Logistic Regression Multiclass

# load the package
library(VGAM)
# load data
data(iris)
# fit model
fit <- vglm(Species~., family=multinomial, data=iris)
# summarize the fit
print(fit)
# make predictions
probabilities <- predict(fit, iris[,1:4], type="response")
predictions <- apply(probabilities, 1, which.max)
predictions[which(predictions=="1")] <- levels(iris$Species)[1]
predictions[which(predictions=="2")] <- levels(iris$Species)[2]
predictions[which(predictions=="3")] <- levels(iris$Species)[3]
# summarize accuracy
table(predictions, iris$Species)



```
## e) NonLinear Classification {@e) NonLinear Classification}
```{r non-linear-classification, message=FALSE, warning=FALSE}
# Random Forest

# load the package
library(randomForest)
# load data
data(iris)
# fit model
fit <- randomForest(Species~., data=iris)
# summarize the fit
print(fit)
# make predictions
predictions <- predict(fit, iris[,1:4])
# summarize accuracy
table(predictions, iris$Species)

# k-Nearest Neighbors

# load the package
library(caret)
data(iris)
# fit model
fit <- knn3(Species~., data=iris, k=5)
# summarize the fit
print(fit)
# make predictions
predictions <- predict(fit, iris[,1:4], type="class")
# summarize accuracy
table(predictions, iris$Species)


# Naive Bayes

# load the package
library(e1071)
data(iris)
# fit model
fit <- naiveBayes(Species~., data=iris)
# summarize the fit
print(fit)
# make predictions
predictions <- predict(fit, iris[,1:4])
# summarize accuracy
table(predictions, iris$Species)

# Classification and Regression Trees

# load the package
library(rpart)
# load data
data(iris)
# fit model
fit <- rpart(Species~., data=iris)
# summarize the fit
print(fit)
# make predictions
predictions <- predict(fit, iris[,1:4], type="class")
# summarize accuracy
table(predictions, iris$Species)

# PART

# load the package
library(RWeka)
# load data
data(iris)
# fit model
fit <- PART(Species~., data=iris)
# summarize the fit
print(fit)
# make predictions
predictions <- predict(fit, iris[,1:4])
# summarize accuracy
table(predictions, iris$Species)

# Classification and Regression Trees

# load the package
library(rpart)
# load data
data(iris)
# fit model
fit <- rpart(Species~., data=iris)
# summarize the fit
print(fit)
# make predictions
predictions <- predict(fit, iris[,1:4], type="class")
# summarize accuracy
table(predictions, iris$Species)

# C5.0

# load the package
library(C50)
# load data
data(iris)
# fit model
fit <- C5.0(Species~., data=iris, trials=10)
# summarize the fit
print(fit)
# make predictions
predictions <- predict(fit, iris[,1:4])
# summarize accuracy
table(predictions, iris$Species)

# Flexible Discriminant Analysis

# load the package
library(mda)
data(iris)
# fit model
fit <- fda(Species~., data=iris)
# summarize the fit
print(fit)
# make predictions
predictions <- predict(fit, iris[,1:4])
# summarize accuracy
table(predictions, iris$Species)

# Gradient Boosted Machine

# load the package
library(gbm)
# load data
data(iris)
# fit model
fit <- gbm(Species~., data=iris, distribution="multinomial")
# summarize the fit
print(fit)
# make predictions
probabilities <- predict(fit, iris[,1:4], n.trees=1)
predictions <-  colnames(probabilities)[apply(probabilities, 1, which.max)]
# summarize accuracy
table(predictions, iris$Species)

# Regularized Discriminant Analysis

# load the package
library(klaR)
data(iris)
# fit model
fit <- rda(Species~., data=iris, gamma=0.05, lambda=0.01)
# summarize the fit
print(fit)
# make predictions
predictions <- predict(fit, iris[,1:4])$class
# summarize accuracy
table(predictions, iris$Species)

# Support Vector Machine

# load the package
library(kernlab)
data(iris)
# fit model
fit <- ksvm(Species~., data=iris)
# summarize the fit
print(fit)
# make predictions
predictions <- predict(fit, iris[,1:4], type="response")
# summarize accuracy
table(predictions, iris$Species)

# Feed Forward Neural Network

# load the package
library(nnet)
data(iris)
# fit model
fit <- nnet(Species~., data=iris, size=4, decay=0.0001, maxit=500)
# summarize the fit
print(fit)
# make predictions
predictions <- predict(fit, iris[,1:4], type="class")
# summarize accuracy
table(predictions, iris$Species)

# Quadratic Discriminant Analysis

# load the package
library(MASS)
data(iris)
# fit model
fit <- qda(Species~., data=iris)
# summarize the fit
print(fit)
# make predictions
predictions <- predict(fit, iris[,1:4])$class
# summarize accuracy
table(predictions, iris$Species)

# Mixture Discriminant Analysis

# load the package
library(mda)
data(iris)
# fit model
fit <- mda(Species~., data=iris)
# summarize the fit
print(fit)
# make predictions
predictions <- predict(fit, iris[,1:4])
# summarize accuracy
table(predictions, iris$Species)

# C4.5

# load the package
library(RWeka)
# load data
data(iris)
# fit model
fit <- J48(Species~., data=iris)
# summarize the fit
print(fit)
# make predictions
predictions <- predict(fit, iris[,1:4])
# summarize accuracy
table(predictions, iris$Species)

# Bagging CART

# load the package
library(ipred)
# load data
data(iris)
# fit model
fit <- bagging(Species~., data=iris)
# summarize the fit
print(fit)
# make predictions
predictions <- predict(fit, iris[,1:4], type="class")
# summarize accuracy
table(predictions, iris$Species)


```


## f) Optimization {@f) Optimization}
```{r Optimization, message=FALSE, warning=FALSE}
# BFGS

# definition of the 2D Rosenbrock function, optima is at (1,1)
rosenbrock <- function(v) {
	(1 - v[1])^2 + 100 * (v[2] - v[1]*v[1])^2
}

# definition of the gradient of the 2D Rosenbrock function
derivative <- function(v) {
	c(-400 * v[1] * (v[2] - v[1]*v[1]) - 2 * (1 - v[1]),
	  200 * (v[2] - v[1]*v[1]))
}

# locate the minimum of the function using the BFGS method
result <- optim(
	c(runif(1,-3,3), runif(1,-3,3)), # start at a random position
	rosenbrock, # the function to minimize
	derivative, # no function gradient
	method="BFGS", # use the BFGS method
	control=c( # configure BFGS
		maxit=100, # maximum iterations of 100
		reltol=1e-8)) # response tolerance over-one step

# summarise results
print(result$par) # the coordinate of the minimim
print(result$value) # the function response of the minimum
print(result$counts) # the number of function calls performed

# dispaly the function as a contour plot
x <- seq(-3, 3, length.out=100)
y <- seq(-3, 3, length.out=100)
z <- rosenbrock(expand.grid(x, y))
contour(x, y, matrix(log10(z), length(x)), xlab="x", ylab="y")
# draw the optima as a point
points(result$par[1], result$par[2], col="red", pch=19)
# draw a square around the optima to highlight it
rect(result$par[1]-0.2, result$par[2]-0.2, result$par[1]+0.2,
	 result$par[2]+0.2, lwd=2)

# Conjugate Gradient method

# definition of the 2D Rosenbrock function, optima is at (1,1)
rosenbrock <- function(v) {
	(1 - v[1])^2 + 100 * (v[2] - v[1]*v[1])^2
}

# definition of the gradient of the 2D Rosenbrock function
derivative <- function(v) {
	c(-400 * v[1] * (v[2] - v[1]*v[1]) - 2 * (1 - v[1]),
	  200 * (v[2] - v[1]*v[1]))
}

# locate the minimum of the function using the Conjugate Gradient method
result <- optim(
	c(runif(1,-3,3), runif(1,-3,3)), # start at a random position
	rosenbrock, # the function to minimize
	derivative, # no function gradient
	method="CG", # use the Conjugate Gradient method
	control=c( # configure Conjugate Gradient
		maxit=100, # maximum iterations of 100
		reltol=1e-8, # response tolerance over-one step
		type=2)) # use the Polak-Ribiere update method

# summarise results
print(result$par) # the coordinate of the minimim
print(result$value) # the function response of the minimum
print(result$counts) # the number of function calls performed

# dispaly the function as a contour plot
x <- seq(-3, 3, length.out=100)
y <- seq(-3, 3, length.out=100)
z <- rosenbrock(expand.grid(x, y))
contour(x, y, matrix(log10(z), length(x)), xlab="x", ylab="y")
# draw the optima as a point
points(result$par[1], result$par[2], col="red", pch=19)
# draw a square around the optima to highlight it
rect(result$par[1]-0.2, result$par[2]-0.2, result$par[1]+0.2,
	 result$par[2]+0.2, lwd=2)

# Golden Section Line Search

# define a 1D basin function, optima at f(0)=0
basin <- function(x) {
	x[1]^2
}

# # locate the minimum of the function using a Golden Section Line Search
result <- optimize(
	basin, # the function to be minimized
	c(-5, 5), # the bounds on the function paramter
	maximum=FALSE, # we are concerned with the function minima
	tol=1e-8) # the size of the final bracketing

# display the results
print(result$minimum) #function parameter
print(result$objective) # function response

# plot the function
x <- seq(-5, 5, length.out=100)
y <- basin(expand.grid(x))
plot(x, y, xlab="x",ylab="f(x)", type="l")
# plot the solution as a point
points(result$minimum, result$objective, col="red", pch=19)
# draw a square around the optima to highlight it
rect(result$minimum-0.3, result$objective-0.7, result$minimum+0.3,
	 result$objective+0.7, lwd=2)

# Gradient Descent method

# define a 2D basin function, optima is at (0,0)
basin <- function(x) {
	x[1]^2 + x[2]^2
}

# define the derivative for a 2D basin function
derivative <- function(x) {
	c(2*x[1], 2*x[2])
}

# definition of the gradient descent method in 2D
gradient_descent <- function(func, derv, start, step=0.05, tol=1e-8) {
	pt1 <- start
	grdnt <- derv(pt1)
	pt2 <- c(pt1[1] - step*grdnt[1], pt1[2] - step*grdnt[2])
	while (abs(func(pt1)-func(pt2)) > tol) {
		pt1 <- pt2
		grdnt <- derv(pt1)
		pt2 <- c(pt1[1] - step*grdnt[1], pt1[2] - step*grdnt[2])
		print(func(pt2)) # print progress
	}
	pt2 # return the last point
}

# locate the minimum of the function using the Gradient Descent method
result <- gradient_descent(
	basin, # the function to optimize
	derivative, # the gradient of the function
	c(runif(1,-3,3), runif(1,-3,3)), # start point of the search
	0.05, # step size (alpha)
	1e-8) # relative tolerance for one step

# display a summary of the results
print(result) # coordinate of fucntion minimum
print(basin(result)) # response of fucntion minimum

# dispaly the function as a contour plot
x <- seq(-3, 3, length.out=100)
y <- seq(-3, 3, length.out=100)
z <- basin(expand.grid(x, y))
contour(x, y, matrix(z, length(x)), xlab="x",ylab="y")
# draw the optima as a point
points(result[1], result[2], col="red", pch=19)
# draw a square around the optima to highlight it
rect(result[1]-0.2, result[2]-0.2, result[1]+0.2,
	result[2]+0.2, lwd=2)

# Nelder-Mead method

# definition of the 2D Rosenbrock function, optima is at (1,1)
rosenbrock <- function(v) {
	(1 - v[1])^2 + 100 * (v[2] - v[1]*v[1])^2
}

# locate the minimum of the function using the Nelder-Mead method
result <- optim(
	c(runif(1,-3,3), runif(1,-3,3)), # start at a random position
	rosenbrock, # the function to minimize
	NULL, # no function gradient
	method="Nelder-Mead", # use the Nelder-Mead method
	control=c( # configure Nelder-Mead
		maxit=100, # maximum iterations of 100
		reltol=1e-8, # response tolerance over-one step
		alpha=1.0, # reflection factor
		beta=0.5, # contraction factor
		gamma=2.0)) # expansion factor

# summarise results
print(result$par) # the coordinate of the minimim
print(result$value) # the function response of the minimum
print(result$counts) # the number of function calls performed

# dispaly the function as a contour plot
x <- seq(-3, 3, length.out=100)
y <- seq(-3, 3, length.out=100)
z <- rosenbrock(expand.grid(x, y))
contour(x, y, matrix(log10(z), length(x)), xlab="x",ylab="y")
# draw the optima as a point
points(result$par[1], result$par[2], col="red", pch=19)
# draw a square around the optima to highlight it
rect(result$par[1]-0.2, result$par[2]-0.2, result$par[1]+0.2,
	 result$par[2]+0.2, lwd=2)

```
## g) Caret Implem. Reg and Binary {g) Caret Implemented Regression and Binary}
```{r caret-regression-binary-algorithms, message=FALSE, warning=FALSE}
# Regression Algorithms

# Demonstrate minimal usage of the most popular algorithms with caret.

# load libraries
library(caret)
library(doMC)
registerDoMC(cores=4)

# load data
data(longley)
dataset <- longley
x <- dataset[,1:6]
y <- dataset[,7]

# prepare simple test suite
control <- trainControl(method="cv", number=5)
seed <- 7
metric <- "RMSE"

# Linear Regresson Models

# Linear regression
set.seed(seed)
fit.lm <- train(Employed~., data=dataset, method="lm", metric=metric, preProc=c("center", "scale"), trControl=control)
# Robust Linear Model
set.seed(seed)
fit.rlm <- train(Employed~., data=dataset, method="rlm", metric=metric, preProc=c("center", "scale"), trControl=control)
# Partial Least Squares
set.seed(seed)
fit.pls <- train(Employed~., data=dataset, method="pls", metric=metric, preProc=c("center", "scale"), trControl=control)
# Elasticnet
set.seed(seed)
fit.enet <- train(Employed~., data=dataset, method="enet", metric=metric, preProc=c("center", "scale"), trControl=control)
# glmnet
set.seed(seed)
fit.glmnet <- train(Employed~., data=dataset, method="glmnet", metric=metric, preProc=c("center", "scale"), trControl=control)
# Least Angle Regression
set.seed(seed)
fit.lars <- train(Employed~., data=dataset, method="lars", metric=metric, preProc=c("center", "scale"), trControl=control)


# Nonlinear Regresson Models

# Neural Network
set.seed(seed)
fit.nnet <- train(Employed~., data=dataset, method="nnet", metric=metric, preProc=c("center", "scale"), trControl=control, trace=FALSE)
# Model Averaged Neural Network
set.seed(seed)
fit.avNNet <- train(Employed~., data=dataset, method="avNNet", metric=metric, preProc=c("center", "scale"), trControl=control, trace=FALSE)
# Multivariate Adaptive Regression Spline
set.seed(seed)
tunegrid <- expand.grid(.degree=1:2, .nprune=2:38)
fit.earth <- train(Employed~., data=dataset, method="earth", metric=metric, tuneGrid=tunegrid, preProc=c("center", "scale"), trControl=control)
# Support Vector Machines with Radial Basis Function Kernel
set.seed(seed)
fit.svmRadial <- train(Employed~., data=dataset, method="svmRadial", metric=metric, preProc=c("center", "scale"), trControl=control)
# k-Nearest Neighbors
set.seed(seed)
tunegrid <- expand.grid(.k=1:20)
fit.knn <- train(Employed~., data=dataset, method="knn", metric=metric, tuneGrid=tunegrid, preProc=c("center", "scale"), trControl=control)


# Regression Trees

# CART
set.seed(seed)
fit.rpart <- train(Employed~., data=dataset, method="rpart", metric=metric, trControl=control)
# Conditional Inference Tree
set.seed(seed)
fit.ctree <- train(Employed~., data=dataset, method="ctree", metric=metric, trControl=control)
# Model Tree
set.seed(seed)
fit.M5 <- train(Employed~., data=dataset, method="M5", metric=metric, trControl=control)
# Model Rules
set.seed(seed)
fit.M5Rules <- train(Employed~., data=dataset, method="M5Rules", metric=metric, trControl=control)


# Ensembles

# Bagged CART
set.seed(seed)
fit.treebag <- train(Employed~., data=dataset, method="treebag", metric=metric, trControl=control)
# Random Forest
set.seed(seed)
fit.rf <- train(Employed~., data=dataset, method="rf", metric=metric, trControl=control)
# Stochastic Gradient Boosting
set.seed(seed)
tunegrid <- expand.grid(.interaction.depth=seq(1,7,by=2), .n.trees=seq(100,1000,by=60), .shrinkage=c(0.01, 0.1), .n.minobsinnode=c(1,3,5,7,9))
fit.gbm <- train(Employed~., data=dataset, method="gbm", metric=metric, tuneGrid=tunegrid, trControl=control, verbose=FALSE)
# Cubist
set.seed(seed)
fit.cubist <- train(Employed~., data=dataset, method="cubist", metric=metric, trControl=control)

# Quantile Random Forest
set.seed(seed)
fit.qrf <- train(diabetes~., data=dataset, method="qrf", metric=metric, trControl=control)
# Quantile Regression Neural Network
set.seed(seed)
fit.qrnn <- train(diabetes~., data=dataset, method="qrnn", metric=metric, trControl=control)

# Binary Classification Algorithms

# Demonstrate minimal usage of the most popular algorithms with caret.

# load libraries
library(mlbench)
library(caret)
library(doMC)
registerDoMC(cores=8)

# load data
data(PimaIndiansDiabetes)
dataset <- PimaIndiansDiabetes
x <- dataset[,1:8]
y <- dataset[,9]

# prepare simple test suite
control <- trainControl(method="cv", number=5)
seed <- 7
metric <- "Accuracy"

# Linear Models


# Linear Discriminant Analysis
set.seed(seed)
fit.lda <- train(diabetes~., data=dataset, method="lda", metric=metric, preProc=c("center", "scale"), trControl=control)
# Logistic Regression
set.seed(seed)
fit.glm <- train(diabetes~., data=dataset, method="glm", metric=metric, preProc=c("center", "scale"), trControl=control)
# Penalized Logistic Regression
set.seed(seed)
fit.plr <- train(diabetes~., data=dataset, method="plr", metric=metric, preProc=c("center", "scale"), trControl=control)
# Ordered Logistic or Probit Regression
set.seed(seed)
fit.polr <- train(diabetes~., data=dataset, method="polr", metric=metric, preProc=c("center", "scale"), trControl=control)
# Partial Least Squares
set.seed(seed)
fit.pls <- train(diabetes~., data=dataset, method="pls", metric=metric, preProc=c("center", "scale"), trControl=control)
# GLMNET
set.seed(seed)
tunegrid <- expand.grid(.alpha=seq(0,1,by=0.1), .lambda=c(1e-1, 1e-2, 1e-3, 1e-4, 1e-5, 1e-6))
fit.glmnet <- train(diabetes~., data=dataset, method="glmnet", metric=metric, tuneGrid=tunegrid, preProc=c("center", "scale"), trControl=control)
# sparce LDA
set.seed(seed)
fit.sda <- train(diabetes~., data=dataset, method="sparseLDA", metric=metric, preProc=c("center", "scale"), trControl=control)
# Nearest Shrunken Centroids
set.seed(seed)
tunegrid <- expand.grid(.threshold=seq(0,1,by=0.1))
fit.pam <- train(diabetes~., data=dataset, method="pam", metric=metric, tuneGrid=tunegrid, preProc=c("center", "scale"), trControl=control)



# Non-Linear Models

# MDA
set.seed(seed)
tunegrid <- expand.grid(.subclasses=seq(1:8))
fit.mda <- train(diabetes~., data=dataset, method="mda", metric=metric, tuneGrid=tunegrid, preProc=c("center", "scale"), trControl=control)
# RDA
set.seed(seed)
tunegrid <- expand.grid(.gamma=c(0, 0.5, 1), .lambda=c(0, 0.5, 1))
fit.rda <- train(diabetes~., data=dataset, method="rda", metric=metric, tuneGrid=tunegrid, preProc=c("center", "scale"), trControl=control)
# sparce MDA
set.seed(seed)
tunegrid <- expand.grid(.NumVars=c(2), .lambda=c(0, 0.01), .R=c(2))
fit.smda <- train(diabetes~., data=dataset, method="smda", metric=metric, tuneGrid=tunegrid, preProc=c("center", "scale"), trControl=control)
# Neural net
set.seed(seed)
tunegrid <- expand.grid(.size=seq(1:10), .decay=c(0, .1, 1, 2))
maxSize <- max(tunegrid$.size)
numWeights <- 1*(maxSize * (length(dataset) + 1) + maxSize + 1)
maxit <- 2000
fit.nnet <- train(diabetes~., data=dataset, method="nnet", metric=metric, tuneGrid=tunegrid, preProc=c("center", "scale", "spatialSign"), trControl=control, trace=FALSE, maxit=maxit, MaxNWts=numWeights)
# Multi-Layer Perceptron
set.seed(seed)
fit.mlp <- train(diabetes~., data=dataset, method="mlp", metric=metric, preProc=c("center", "scale"), trControl=control)
# Radial Basis Function Network
set.seed(seed)
fit.rbf <- train(diabetes~., data=dataset, method="rbf", metric=metric, preProc=c("center", "scale"), trControl=control)
# FDA
set.seed(seed)
fit.fda <- train(diabetes~., data=dataset, method="fda", metric=metric, preProc=c("center", "scale"), trControl=control)
# Support Vector Machines with Linear Kernel
set.seed(seed)
sigma <- sigest(as.matrix(x))
tunegrid <- expand.grid(.C=2^(seq(-4, 4)))
fit.svmLinear <- train(diabetes~., data=dataset, method="svmLinear", metric=metric, tuneGrid=tunegrid, preProc=c("center", "scale"), trControl=control)
# Support Vector Machines with Polynomial Kernel
set.seed(seed)
fit.svmPoly <- train(diabetes~., data=dataset, method="svmPoly", metric=metric, preProc=c("center", "scale"), trControl=control, fit=FALSE)
# Support Vector Machines with Radial Basis Function Kernel
set.seed(seed)
fit.svmRadialCost <- train(diabetes~., data=dataset, method="svmRadialCost", metric=metric, preProc=c("center", "scale"), trControl=control, fit=FALSE)
# Support Vector Machines with Radial Basis Function Kernel
set.seed(seed)
sigma <- sigest(as.matrix(x))
tunegrid <- expand.grid(.sigma=sigma, .C=2^(seq(-4, 4)))
fit.svmRadial <- train(diabetes~., data=dataset, method="svmRadial", metric=metric, tuneGrid=tunegrid, preProc=c("center", "scale"), trControl=control, fit=FALSE)
# kNN
set.seed(seed)
tunegrid <- expand.grid(.k=c(4*(0:5)+1, 20*(1:5)+1, 50*(2:9)+1))
fit.knn <- train(diabetes~., data=dataset, method="knn", metric=metric, tuneGrid=tunegrid, preProc=c("center", "scale"), trControl=control)
# naive bayes
set.seed(seed)
tunegrid <- expand.grid(.fL=c(0, 1, 2, 3), .usekernel=TRUE)
fit.nb <- train(diabetes~., data=dataset, method="nb", metric=metric, tuneGrid=tunegrid, trControl=control)
# Learning Vector Quantization
set.seed(seed)
fit.lvq <- train(diabetes~., data=dataset, method="lvq", metric=metric, trControl=control)
# Self-Organizing Maps
set.seed(seed)
fit.xyf <- train(diabetes~., data=dataset, method="xyf", metric=metric, trControl=control)



# Trees


# CART
set.seed(seed)
tunegrid <- expand.grid(.cp=seq(0,0.1,by=0.01))
fit.cart <- train(diabetes~., data=dataset, method="rpart", metric=metric, tuneGrid=tunegrid, trControl=control)
# C4.5 (AKA C4.8 or J48)
set.seed(seed)
fit.j48 <- train(diabetes~., data=dataset, method="J48", metric=metric, trControl=control)
# Logistic Model Trees
set.seed(seed)
fit.LMT <- train(diabetes~., data=dataset, method="LMT", metric=metric, trControl=control)



# Rules

# PART Rule-Based Classifier
set.seed(seed)
fit.part <- train(diabetes~., data=dataset, method="PART", metric=metric, trControl=control)
# Rule-Based Classifier
set.seed(seed)
fit.JRIP <- train(diabetes~., data=dataset, method="JRip", metric=metric, trControl=control)



# Boosting Ensemble Algorithms

# AdaBoost.M1
set.seed(seed)
fit.AdaBoost.M1 <- train(diabetes~., data=dataset, method="AdaBoost.M1", metric=metric, trControl=control)
# Bagged AdaBoost
set.seed(seed)
fit.AdaBag <- train(diabetes~., data=dataset, method="AdaBag", metric=metric, trControl=control)
# Boosted Classification Trees
set.seed(seed)
fit.ada <- train(diabetes~., data=dataset, method="ada", metric=metric, trControl=control)
# Boosted Generalized Additive Model
set.seed(seed)
fit.gamboost <- train(diabetes~., data=dataset, method="gamboost", metric=metric, trControl=control)
# Boosted Generalized Linear Model
set.seed(seed)
fit.glmboost <- train(diabetes~., data=dataset, method="glmboost", metric=metric, trControl=control)
# Boosted Linear Model
set.seed(seed)
fit.BstLm <- train(diabetes~., data=dataset, method="BstLm", metric=metric, trControl=control)
# Boosted Logistic Regression
set.seed(seed)
fit.LogitBoost <- train(diabetes~., data=dataset, method="LogitBoost", metric=metric, trControl=control)
# Boosted Smoothing Spline
set.seed(seed)
fit.bstSm <- train(diabetes~., data=dataset, method="bstSm", metric=metric, trControl=control)
# Boosted Tree
set.seed(seed)
fit.blackboost <- train(diabetes~., data=dataset, method="blackboost", metric=metric, trControl=control)
# Boosted Tree
set.seed(seed)
fit.bstTree <- train(diabetes~., data=dataset, method="bstTree", metric=metric, trControl=control)
# C5.0
set.seed(seed)
tunegrid <- expand.grid(.trials=c(5,10,15,20), .model=c("rules", "rules"), .winnow=c(TRUE, FALSE))
fit.c50 <- train(diabetes~., data=dataset, method="C5.0", metric=metric, tuneGrid=tunegrid, trControl=control)
# Cost-Sensitive C5.0
set.seed(seed)
fit.C5.0Cost <- train(diabetes~., data=dataset, method="C5.0Cost", metric=metric, trControl=control)
# eXtreme Gradient Boosting
set.seed(seed)
fit.xgbLinear <- train(diabetes~., data=dataset, method="xgbLinear", metric=metric, trControl=control)
# eXtreme Gradient Boosting
set.seed(seed)
fit.xgbTree <- train(diabetes~., data=dataset, method="xgbTree", metric=metric, trControl=control)
# Stochastic Gradient Boosting
set.seed(seed)
tunegrid <- expand.grid(.n.trees=c(5, 100, 500), .interaction.depth=c(1, 3, 5, 7, 9), .shrinkage=c(0, 1e-1, 1e-2, 1e-3, 1e-4), .n.minobsinnode=c(5, 10))
fit.gbm <- train(diabetes~., data=dataset, method="gbm", metric=metric, tuneGrid=tunegrid, trControl=control, verbose=FALSE)



# Bagged Ensemble Algorithms

# Bagged CART
set.seed(seed)
fit.treebag <- train(diabetes~., data=dataset, method="treebag", metric=metric, trControl=control)
# Bagged Flexible Discriminant Analysis
set.seed(seed)
fit.bagFDA <- train(diabetes~., data=dataset, method="bagFDA", metric=metric, trControl=control)
# Bagged Logic Regression
set.seed(seed)
fit.logicBag <- train(diabetes~., data=dataset, method="logicBag", metric=metric, trControl=control)
# Bagged MARS
set.seed(seed)
fit.bagEarthGCV <- train(diabetes~., data=dataset, method="bagEarthGCV", metric=metric, trControl=control)
# Bagged MARS using gCV Pruning
set.seed(seed)
fit.bagEarth <- train(diabetes~., data=dataset, method="bagEarth", metric=metric, trControl=control)
# Bagged Model
set.seed(seed)
fit.bag <- train(diabetes~., data=dataset, method="bag", metric=metric, trControl=control)
# Conditional Inference Random Forest
set.seed(seed)
fit.cforest <- train(diabetes~., data=dataset, method="cforest", metric=metric, trControl=control)
# Ensembles of Generalized Lienar Models
set.seed(seed)
fit.randomGLM <- train(diabetes~., data=dataset, method="randomGLM", metric=metric, trControl=control)
# Model Averaged Neural Network
set.seed(seed)
fit.avNNet <- train(diabetes~., data=dataset, method="avNNet", metric=metric, trControl=control)
# Parallel Random Forest
set.seed(seed)
fit.parRF <- train(diabetes~., data=dataset, method="parRF", metric=metric, trControl=control)
# Random Ferns
set.seed(seed)
fit.rFerns <- train(diabetes~., data=dataset, method="rFerns", metric=metric, trControl=control)
# Random Forest
set.seed(seed)
fit.ranger <- train(diabetes~., data=dataset, method="ranger", metric=metric, trControl=control)
# Random Forest (classical)
set.seed(seed)
fit.rf <- train(diabetes~., data=dataset, method="rf", metric=metric, trControl=control)
# Random Forest by Randomization
set.seed(seed)
fit.extraTrees <- train(diabetes~., data=dataset, method="extraTrees", metric=metric, trControl=control)
# Random Forest Rule-Based Model
set.seed(seed)
fit.rfRules <- train(diabetes~., data=dataset, method="rfRules", metric=metric, trControl=control)
# Regularized Random Forest
set.seed(seed)
fit.RRF <- train(diabetes~., data=dataset, method="RRF", metric=metric, trControl=control)
# Regularized Random Forest
set.seed(seed)
fit.RRFglobal <- train(diabetes~., data=dataset, method="RRFglobal", metric=metric, trControl=control)
# Weighted Subspace Random Forest
set.seed(seed)
fit.wsrf <- train(diabetes~., data=dataset, method="wsrf", metric=metric, trControl=control)
```
